ðŸ©º Clinical NER for Low-Resource Languages (Bengali)

> Designed a scalable annotation pipeline using prompt engineering + chain-of-thought reasoning with GPT-4.1 to auto-label noisy Bengali clinical text.

> Chain-of-Thought prompting tripled token-level F1 vs. plain prompting (0.120 â†’ 0.335).

> Trained transformer models (mBERT, BanglaBERT, BanglishBERT) on synthetic labels, with BanglishBERT reaching 46.0 macro-F1, closing ~70% of the gap to human-labeled data.

> Demonstrated that prompt design matters more than model size for label quality.

> Provides a practical alternative to expensive expert annotation in healthcare NLP.
